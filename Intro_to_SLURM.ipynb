{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a7eb3a-7010-444d-81d5-9cda8bd8e935",
   "metadata": {},
   "source": [
    "<img src=\"New_accre_logo.png\" width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e08c4-7534-4876-b642-dc6e7ad8e8ab",
   "metadata": {},
   "source": [
    "# 1.0 Overview of the Class Environment & SLURM\n",
    "\n",
    "This notebook will introduce the basic knowledge of using ACCRE cluster at Vanderbilt. You will have an overview of the Class Environment configured as an ACCRE compute cluster. In addition, you will experiment with basic commands of the [SLURM cluster management](https://slurm.schedmd.com/overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad0a52-9fb4-4e86-96d7-959078c80ffc",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Understand the hardware configuration available for the class\n",
    "* Understand the basics commands for jobs submissions with SLURM\n",
    "* Run simple test scripts allocating different GPU resources\n",
    "* Connect interactively to a compute node and observe available resources\n",
    "\n",
    "**[1.1 The Hardware Configuration Overview](#1.1-The-Hardware-Configuration-Overview)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.1 Check The Available CPUs](#1.1.1-Check-The-Available-CPUs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.2 Check the Available GPUs](#1.1.2-Check-The-Available-GPUs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.3 Check The Interconnect Topology](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.4 Bandwidth & Connectivity Tests](#1.1.4-Bandwidth-and-Connectivity-Tests)<br>\n",
    "**[1.2 Basic SLURM Commands](#1.2-Basic-SLURM-Commands)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.1 Check the SLURM Configuration](#1.2.1-Check-the-SLURM-Configuration)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.2 Submit Jobs Using SRUN Command](#1.2.2-Submit-jobs-using-SRUN-Command)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.3 Submit Jobs Using SBATCH Command](#1.2.3-Submit-jobs-using-SBATCH-Command])<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.4 Exercise: Submit Jobs Using SBATCH Command Requesting More Resources](#1.2.4-Exercise-Submit-jobs-using-SBATCH-Command])<br>\n",
    "**[1.3 Run Interactive Sessions](#1.3-Run-Interactive-Sessions)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd503da-0e6f-4497-bafa-af94400f8644",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.1 The Hardware Configuration Overview\n",
    "\n",
    "\n",
    "A modern AI cluster is a type of infrastructure designed for optimal Deep Learning model development. ACCRE has GPU servers suitable for scalable AI development. Click the link to learn more about [ACCRE GPUs](https://help.accre.vanderbilt.edu/index.php?title=GPUs_at_ACCRE).\n",
    "\n",
    "Different deliveries of this course may have different hardware configurations. For benchmarking purposes, we will be using 2 A100s/A6000 as a reference that are NVLink. NVLink is a flexible and scalable interconnect technology, enabling multiple GPUs with a variety of interconnect topologies and bandwidths.\n",
    "\n",
    "<img  src=\"nvlink_configurability-624x289.png\" width=\"1000\"/>\n",
    "\n",
    "The hardware for this class has already been configured as a GPU cluster unit for Deep Learning. The cluster is organized as compute units (nodes) that can be allocated using a Cluster Manager (example SLURM). Among the hardware components, the cluster includes CPUs (Central Processing Units), GPUs (Graphics Processing Units), storage and networking.\n",
    "\n",
    "Let's look at the GPUs, CPUs and network design available in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6c341-3f40-40b2-9cce-3f373852aa41",
   "metadata": {},
   "source": [
    "## 1.1.1 Check the Available CPUs \n",
    "\n",
    "We can check the CPU information of the system using the `lscpu` command. \n",
    "\n",
    "This example of outputs shows that there are 12 CPU cores of the `x86_64` from Intel.\n",
    "```\n",
    "Architecture:                    x86_64\n",
    "Core(s) per socket:              6\n",
    "Model name:                      Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz\n",
    "```\n",
    "For a complete description of the CPU processor architecture, check the `/proc/cpuinfo` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae96427d-9752-4cf0-a035-5dcb7d4f6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information CPUs\n",
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9ea5d-e89c-4ca4-8814-ce33a20245e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the number of CPU cores\n",
    "!grep 'cpu cores' /proc/cpuinfo | uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea15da6-dae8-41bc-8df6-b815b14cfbff",
   "metadata": {},
   "source": [
    "## 1.1.2 Check the Available  GPUs \n",
    "\n",
    "The NVIDIA System Management Interface `nvidia-smi` is a command for monitoring NVIDIA GPU devices. Several key details are listed such as the CUDA and  GPU driver versions, the number and type of GPUs available, the GPU memory each, running GPU process, etc.\n",
    "\n",
    "In the following example, `nvidia-smi` command shows that there are GPUs, each with approximately 80GB of memory. \n",
    "\n",
    "<img  src=\"Nvidasmi.png\" width=\"600\"/>\n",
    "\n",
    "For more details, refer to the [nvidia-smi documentation](https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499cc2d-61ba-4161-be5b-d0b220c47ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about GPUs\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a384f8b6-89e9-4828-aec2-b16f939f394a",
   "metadata": {},
   "source": [
    "## 1.1.3 Check the Available Interconnect Topology \n",
    "\n",
    "\n",
    "\n",
    "The multi-GPU system configuration needs a fast and scalable interconnect. [NVIDIA NVLink technology](https://www.nvidia.com/en-us/data-center/nvlink/) is a direct GPU-to-GPU interconnect providing high bandwidth and improving scalability for multi-GPU systems.\n",
    "\n",
    "To check the available interconnect topology, we can use `nvidia-smi topo --matrix` command. In this class, we should get 4 NVLinks per GPU device. \n",
    "\n",
    "```\n",
    "        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\n",
    "GPU0     X      NV12    SYS     SYS     0-23            N/A\n",
    "GPU1    NV12     X      SYS     SYS     24-47           N/A\n",
    "GPU2    SYS     SYS      X      NV12    48-71           N/A\n",
    "GPU3    SYS     SYS     NV12     X      72-95           N/A\n",
    "\n",
    "Where X= Self and NV# = Connection traversing a bonded set of # NVLinks\n",
    "```\n",
    "\n",
    "In this environment, notice only 1 link between GPU0 and GPU1, GPU2 while 2 links are shown between GPU0 and GPU3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca53f26-c82b-4e3c-9f3e-3adf802f3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Interconnect Topology \n",
    "!nvidia-smi topo --matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e24d19-3751-4a3e-a6df-f047b270bca6",
   "metadata": {},
   "source": [
    "It is also possible to check the NVLink status and bandwidth using `nvidia-smi nvlink --status` command. You should see similar outputs per device.\n",
    "```\n",
    "GPU 0: Graphics Device\n",
    "\t Link 0: 25 GB/s\n",
    "\t Link 1: 25 GB/s\n",
    "\t Link 2: 25 GB/s\n",
    "\t Link 3: 25 GB/s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bbb4f1-1cee-4dbf-9adc-600aa34beeab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check nvlink status\n",
    "!nvidia-smi nvlink --status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e45b3-60c5-4fef-a370-47fe812fd798",
   "metadata": {},
   "source": [
    "## 1.1.4 Bandwidth & Connectivity Tests\n",
    "\n",
    "\n",
    "NVIDIA provides an application **p2pBandwidthLatencyTest** that demonstrates CUDA Peer-To-Peer (P2P) data transfers between pairs of GPUs by computing bandwidth and latency while enabling and disabling NVLinks. This tool is part of the code samples for CUDA Developers [cuda-samples](https://github.com/NVIDIA/cuda-samples.git). \n",
    "\n",
    "Why it Matters? Models like AF3 rely on rapid access to tensors in memory:\n",
    "   -  High-bandwidth memory (HBM) on GPUs allows faster matrix multiplications and reduces time spent waiting for data.\n",
    "   -  A100 GPUs, for instance, have >1.5 TB/s memory bandwidth, crucial for large protein structures.\n",
    "   -  NVLink (e.g., in DGX systems) enables fast GPU-GPU communication, reducing transfer delays in parallelized training.\n",
    "\n",
    "Example outputs are shown below. Notice the Device to Device (D\\D) bandwidth differences when enabling and disabling NVLinks (P2P).\n",
    "\n",
    "```\n",
    "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1      2      3 \n",
    "     0 1529.61 516.36  20.75  21.54 \n",
    "     1 517.04 1525.88  20.63  21.33 \n",
    "     2  20.32  20.17 1532.61 517.23 \n",
    "     3  20.95  20.83 517.98 1532.61 \n",
    "\n",
    "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1      2      3 \n",
    "     0 1532.61  18.09  20.79  21.52 \n",
    "     1  18.11 1531.11  20.65  21.33 \n",
    "     2  20.32  20.17 1528.12  28.89 \n",
    "     3  20.97  20.82  28.36 1531.11 \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80ea74-575a-44e5-8a51-eb7b4c0f7831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests on GPU pairs using P2P and without P2P \n",
    "#'git clone --depth 1 --branch v11.2 https://github.com/NVIDIA/cuda-samples.git'\n",
    "!cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42098d2b-e150-453b-9164-edc72df6c937",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.2 Basic SLURM Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeba23d-bbe5-4c50-a603-d9dcbaa5c3ac",
   "metadata": {},
   "source": [
    "Now that we've seen how GPUs can communicate with each other over NVLink, let's go over how the hardware resources can be organized into compute nodes. These nodes can be managed by Cluster Manager such as [*Slurm Workload Manager*](https://slurm.schedmd.com/), an open source cluster management and job scheduler system for large and small Linux clusters. \n",
    "\n",
    "\n",
    "For this lab, we have configured a SLURM manager where the 2 available GPUs are partitioned into 2 nodes: **slurmnode1** \n",
    "and **slurmnode2**, each with 2 GPUs. \n",
    "\n",
    "Next, let's see some basic SLURM commands. More SLURM commands can be found in the [SLURM official documentation](https://slurm.schedmd.com/).\n",
    "\n",
    "<img src=\"arch.gif\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a9649-054b-48e0-a75d-3a98fa668d67",
   "metadata": {},
   "source": [
    "## 1.2.1 Check the SLURM Configuration\n",
    "\n",
    "We can check the available resources in the SLURM cluster by running `sinfo`. The output will show that there are 2 nodes in the cluster **slurmnode1** and **slurmnode2**. Both nodes are currently idle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52b7b8-0535-4791-93ab-7a7c2264c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available resources in the cluster\n",
    "!sinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e731f0-3f1c-491c-bbb6-676eb6c3ba09",
   "metadata": {},
   "source": [
    "##  1.2.2 Submit Jobs Using `srun` Command\n",
    "\n",
    "The `srun` command allows to running parallel jobs. \n",
    "\n",
    "The argument **-N** (or *--nodes*) can be used to specify the nodes allocated to a job. It is also possible to allocate a subset of GPUs available within a node by specifying the argument **-G (or --gpus)**.\n",
    "\n",
    "Check out the [SLURM official documentation](https://slurm.schedmd.com/) for more arguments.\n",
    "\n",
    "To test running parallel jobs, let's submit a job that requests 1 node (2 GPUs) and run a simple command on it: `nvidia-smi`. We should see the output of 2 GPUs available in the allocated node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aad880-f0a1-417b-ac01-959de01d7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run nvidia-smi slurm job with 1 node allocation\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f6e97-f7cb-4653-9cbc-c4191be3b97a",
   "metadata": {},
   "source": [
    "## 1.2.3 Submit Jobs Using `sbatch` Command \n",
    "\n",
    "In the previous examples, we allocated resources to run one single command. For more complex jobs, the `sbatch` command allows submitting batch scripts to SLURM by specifying the resources and all environment variables required for executing the job. `sbatch` will transfer the execution to the SLURM Manager after automatically populating the arguments.\n",
    "\n",
    "In the batch script below, `#SBATCH ...` is used to specify resources and other options relating to the job to be executed:\n",
    "\n",
    "```\n",
    "        #!/bin/bash\n",
    "        #SBATCH -N 1                               # Node count to be allocated for the job\n",
    "        #SBATCH --job-name=firstSlurmJob           # Job name\n",
    "        #SBATCH -o /MY/PATH/logs/%j.out           # Outputs log file \n",
    "        #SBATCH -e /MY/PATH/logs/%j.err           # Errors log file\n",
    "\n",
    "        srun -l my_script.sh                       # my SLURM script \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc177942-5ce2-45d2-ba71-3a81a3b3801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chmod +x /dli/code/test.sh\n",
    "# Check the batch script \n",
    "!cat /home/soubasbj/alphafold3/specialrun.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd3d7c-08a8-4301-9c02-123ca8bb9283",
   "metadata": {},
   "source": [
    "To submit this batch script job, let's create an `sbatch` script that initiates the resources to be allocated and submits the test.sh job.\n",
    "\n",
    "The following cell will edit the `test_sbatch.sbatch` script allocating 1 node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30404bb0-1b05-476d-872f-c86501ef5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -l /home/soubasbj/alphafold3/specialrun.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c9d52-0cac-406d-a90d-20608d2121a1",
   "metadata": {},
   "source": [
    "Now let's submit the `sbatch` job and check the SLURM scheduler. The batch script will be queued and executed when the requested resources are available.\n",
    "\n",
    "The `squeue` command shows the running or pending jobs. An output example is shown below: \n",
    "\n",
    "```\n",
    "Submitted batch job **\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "                **  slurmpar soubasbj    admin  R       0:01      1 slurmnode1\n",
    "\n",
    "```\n",
    "\n",
    "It shows the SLURM Job ID, Job's name, the user ID, Job's Status (R=running), running duration and the allocated node name.\n",
    "\n",
    "The following cell submits the `sbatch` job, collects the `JOBID` variable (for querying later the logs) and checks the jobs in the SLURM scheduling queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d84f6c-8cdf-410f-9a27-11787bca5456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submit the job\n",
    "#!sbatch /home/soubasbj/alphafold3/specialrun.sh\n",
    "\n",
    "# Get the JOBID variable\n",
    "#JOBID=!squeue -u soubasbj | grep alphafold | awk '{print $1}'\n",
    "#slurm_job_output='/dli/nemo/logs/'+JOBID[0]+'.out'\n",
    "\n",
    "# check the jobs in the SLURM scheduling queue\n",
    "!squeue -u soubasbj\n",
    "#!scancel 1894053"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d632f-a415-4868-9cde-883875efdc11",
   "metadata": {},
   "source": [
    "The output log file for the executed job (**JOBID.out**) is automatically created to gather the outputs.\n",
    "\n",
    "In our case, we should see the results of `nvidia-smi` command that was executed in the `test.sh` script submitted with 1 node allocation. Let's have a look at execution logs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ed5f3-00b3-4ae7-ad01-0d6b037310d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the execution logs \n",
    "!cat /home/soubasbj/alphafold3/af3-fold_msba_2mg_2atp_noseed_rep1_job_request.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b932e-d9e8-46b5-87cb-3e263a8eaa72",
   "metadata": {},
   "source": [
    "## 1.2.4  Exercise: Submit Jobs Using `sbatch` Command  Requesting More Resources (SKIP)\n",
    "\n",
    "\n",
    "Using what you have learned, submit the previous `test.sh` batch script with the `sbatch` command on **2 nodes** allocation.\n",
    "\n",
    "To do so, you will need to:\n",
    "1. Modify the `test_sbatch.sbatch` script to allocate 2 Nodes \n",
    "2. Submit the script again using `sbatch` command\n",
    "3. Check the execution logs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d44d3-8f14-4c34-b126-9641445503b2",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.3 Run Interactive Sessions \n",
    "\n",
    "Interactive sessions allow to connect directly to a worker node and interact with it through the terminal. \n",
    "\n",
    "The SLURM manager allows to allocate resources in interactive session using the `--pty` argument as follows: `srun -N 1 --pty /bin/bash`. \n",
    "The session is closed when you exit the node or you cancel the interactive session job using the command `scancel JOBID`.\n",
    "\n",
    "\n",
    "Since this is an interactive session, first, we need to launch a terminal window and submit a slurm job allocating resources in interactive mode. To do so, we will need to follow the 3 steps: \n",
    "1. Launch a terminal session\n",
    "2. Check the GPUs resources using the command `nvidia-smi` \n",
    "3. Run an interactive session requesting 1 node by executing `srun -N 1 --pty /bin/bash`\n",
    "4. Check the GPUs resources using the command `nvidia-smi` again \n",
    "\n",
    "Let's run our first interactive job requesting 1 node and check what GPU resources are at our disposal. \n",
    "\n",
    "\n",
    "Notice that while connected to the session, the host name as displayed in the command line changes from \"lab\" (login node name) to \"slurmnode1\" indicating that we are now successfully working on a remote worker node.\n",
    "\n",
    "Run the following cell to get a link to open a terminal session and the instructions to run an interactive session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ac91a-f45c-46bf-b233-647963d2e2fc",
   "metadata": {},
   "source": [
    "<pre>\n",
    "   Step 1: Open a terminal session by following this <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Check the GPUs resources: <font color=\"green\">nvidia-smi</font>\n",
    "   Step 3: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
    "   Step 4: Check the GPUs resources again: <font color=\"green\">nvidia-smi</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3181a-48f5-433f-b272-5566596901f8",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've made it through the first section of the presentation and are ready to begin training models on multiple GPUs. <br>\n",
    "\n",
    "Before moving on, we need to make sure that no jobs are still running or waiting on the SLURM queue. \n",
    "Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b394e-4b56-4e77-bf3c-e7dc2524ad00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb131cdd-cfa9-460e-9bed-45497e9a53e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "#!squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff848e3a-eae4-4ae6-9dee-f46675756adc",
   "metadata": {},
   "source": [
    "Next, we will be running basic model training on different distribution configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
